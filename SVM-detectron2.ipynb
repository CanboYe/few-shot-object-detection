{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n",
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetMapper\n",
    "from detectron2.data import (\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "from detectron2.data.samplers import TrainingSampler\n",
    "from detectron2.modeling.proposal_generator.proposal_utils import add_ground_truth_to_proposals\n",
    "from detectron2.modeling.sampling import subsample_labels\n",
    "from detectron2.structures import Boxes, Instances, pairwise_iou\n",
    "from detectron2.modeling.matcher import Matcher\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "from fsdet.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from fsdet.evaluation import (\n",
    "    COCOEvaluator, DatasetEvaluators, LVISEvaluator, PascalVOCDetectionEvaluator, verify_results)\n",
    "from fsdet.evaluation import (\n",
    "    DatasetEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    "    verify_results,\n",
    ")\n",
    "import cv2\n",
    "import torch, torchvision\n",
    "import logging\n",
    "import os\n",
    "from sklearn import svm\n",
    "from joblib import dump, load\n",
    "from detectron2.layers import nonzero_tuple\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained detection model and Prediect directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models of DNN and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fvcore.common.checkpoint:Loading checkpoint from checkpoints/coco/faster_rcnn/30shot_person_unfreeze_lastfews/model_final.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fvcore.common.checkpoint:Some model parameters or buffers are not found in the checkpoint:\n",
      "  \u001b[34mroi_heads.box_predictor.cls_score.bias\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file('configs/test_unfreeze_lastfews.yaml')\n",
    "model = build_model(cfg)  # returns a torch.nn.Module\n",
    "print(cfg.INPUT.FORMAT)\n",
    "\n",
    "model.eval()\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "# ckpt_file = 'checkpoints/coco/base_model/model_final.pth'\n",
    "ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_person_unfreeze_lastfews/model_final.pth'\n",
    "# ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_person_unfreeze_whole/model_0015999.pth'\n",
    "DetectionCheckpointer(model).load(ckpt_file)\n",
    "\n",
    "clf = load('svm_results/svm_model_finetuned_prop_base.joblib') \n",
    "# clf = load('svm_results/svm_model_finetuned_prop_whole.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "json_dir = 'datasets/coco_experiments/seed1/full_box_30shot_person_trainval.json'\n",
    "image_dir = 'datasets/coco/trainval2014'\n",
    "register_coco_instances(\"30shot_person_train\", {}, json_dir, image_dir)\n",
    "\n",
    "json_dir = 'datasets/coco_experiments/seed1/full_box_1000shot_person_test.json'\n",
    "image_dir = 'datasets/coco/trainval2014'\n",
    "register_coco_instances(\"1000shot_person_test\", {}, json_dir, image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between some dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = build_detection_train_loader(cfg)\n",
    "# data_loader_train_it = iter(data_loader_train)\n",
    "# data = next(data_loader_train_it)\n",
    "# print(len(data))\n",
    "# print(data[0]['image'].shape)\n",
    "# # print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:detectron2.data.datasets.coco:Loaded 239 images in COCO format from datasets/coco_experiments/seed1/full_box_1000shot_person_test.json\n",
      "INFO:detectron2.data.build:Distribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   person   | 1000         |\n",
      "|            |              |\u001b[0m\n",
      "INFO:detectron2.data.dataset_mapper:[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "INFO:detectron2.data.common:Serializing 239 elements to byte tensors and concatenating them all ...\n",
      "INFO:detectron2.data.common:Serialized dataset takes 0.67 MiB\n"
     ]
    }
   ],
   "source": [
    "data_loader_test = build_detection_test_loader(cfg, \"1000shot_person_test\")\n",
    "data_loader_test_it = iter(data_loader_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([3, 800, 1166])\n"
     ]
    }
   ],
   "source": [
    "data = next(data_loader_test_it)\n",
    "print(len(data))\n",
    "print(data[0]['image'].shape)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_gen = T.ResizeShortestEdge(\n",
    "#             [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],\n",
    "#             cfg.INPUT.MAX_SIZE_TEST,\n",
    "#             )\n",
    "# print([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],\n",
    "#             cfg.INPUT.MAX_SIZE_TEST)\n",
    "# data_loader = build_detection_train_loader(DatasetCatalog.get(\"1000shot_person_test\"),\n",
    "#                                            mapper=DatasetMapper(cfg, is_train=False, augmentations=[transform_gen]),\n",
    "#                                            total_batch_size = 10)\n",
    "# data_loader_it = iter(data_loader)\n",
    "# data = next(data_loader_it)\n",
    "# print(len(data))\n",
    "# print(data[0]['image'].shape)\n",
    "# print(data)\n",
    "\n",
    "## the type of dataloader is different 'AspectRatio Grouped Dataset' (don't know the reason) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.utils.data.dataloader.DataLoader\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "print(type(data_loader_test))\n",
    "print(len(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloudlet/anaconda3/envs/fsdet/lib/python3.6/site-packages/detectron2/modeling/roi_heads/fast_rcnn.py:124: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370116979/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  filter_inds = filter_mask.nonzero()\n",
      "INFO:fsdet.evaluation.coco_evaluation:Preparing results for COCO format ...\n",
      "INFO:fsdet.evaluation.coco_evaluation:Saving results to checkpoints/coco/faster_rcnn/test_0215/coco_instances_results.json\n",
      "INFO:fsdet.evaluation.coco_evaluation:Evaluating predictions ...\n",
      "INFO:fsdet.evaluation.coco_evaluation:Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 0.184 | 0.424  | 0.000  | 0.132 | 0.173 | 0.248 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.07s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.001\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n"
     ]
    }
   ],
   "source": [
    "# run one iter\n",
    "evaluator = COCOEvaluator(\"1000shot_person_test\", cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "with torch.no_grad():\n",
    "    inputs = data\n",
    "    outputs = model(inputs)\n",
    "    evaluator.reset()\n",
    "    evaluator.process(inputs, outputs)\n",
    "    results = evaluator.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instances': Instances(num_instances=100, image_height=343, image_width=500, fields=[pred_boxes: Boxes(tensor([[307.4873, 139.3737, 430.1411, 278.2307],\n",
      "        [ 34.8617, 131.7565,  53.4179, 167.0400],\n",
      "        [264.2541, 110.1587, 447.5597, 305.4304],\n",
      "        [228.1001, 113.6795, 278.5739, 158.8312],\n",
      "        [219.2898, 125.5122, 239.9398, 156.1071],\n",
      "        [ 60.2247, 118.4398,  77.9698, 158.3530],\n",
      "        [439.1581, 112.6526, 492.3134, 159.4070],\n",
      "        [245.5859, 112.5854, 263.1079, 165.0826],\n",
      "        [229.1883, 114.7197, 248.3466, 159.9225],\n",
      "        [261.0497, 115.3187, 275.6426, 161.9311],\n",
      "        [137.4505, 124.4982, 237.0562, 156.1336],\n",
      "        [183.7613, 115.2427, 215.6950, 196.0591],\n",
      "        [ 35.8171, 118.0838, 309.2529, 270.4918],\n",
      "        [ 11.0887, 137.1735,  34.2363, 157.8231],\n",
      "        [106.0640,  98.7603, 430.5492, 266.2970],\n",
      "        [ 29.0133, 124.1063, 161.5993, 152.6946],\n",
      "        [170.5462, 117.1802, 230.6380, 171.0150],\n",
      "        [251.7086, 114.5926, 270.1111, 163.9887],\n",
      "        [256.1053, 139.1332, 437.3041, 227.6830],\n",
      "        [222.0238, 117.4699, 298.7926, 144.3130],\n",
      "        [417.0555, 137.9990, 439.7344, 160.7996],\n",
      "        [439.4025, 141.0512, 484.1338, 157.7439],\n",
      "        [131.4241, 114.1229, 173.1355, 158.0024],\n",
      "        [ 38.8256, 123.3477, 247.1997, 173.4967],\n",
      "        [208.2657, 138.1126, 350.5528, 264.1586],\n",
      "        [446.9086, 139.9494, 472.7974, 167.8096],\n",
      "        [173.5692, 122.2436, 190.4790, 159.8766],\n",
      "        [310.2235, 119.6314, 364.9282, 172.0390],\n",
      "        [232.7565, 113.4393, 263.1217, 143.4087],\n",
      "        [213.2637, 124.6019, 314.7388, 151.4747],\n",
      "        [468.5193, 123.2115, 486.6797, 157.1520],\n",
      "        [  9.6507, 137.0539,  63.5124, 156.9392],\n",
      "        [267.8544, 111.7255, 411.8273, 169.4413],\n",
      "        [244.0247, 118.0833, 343.3546, 149.3137],\n",
      "        [301.1504, 131.2338, 330.1938, 157.6487],\n",
      "        [280.5370, 118.0181, 299.7518, 160.2613],\n",
      "        [177.4105, 123.6506, 283.7499, 153.2339],\n",
      "        [316.5724, 135.7185, 360.1125, 283.5716],\n",
      "        [134.6968, 135.2325, 173.9922, 154.9807],\n",
      "        [ 35.2001, 137.3127, 169.5953, 163.1091],\n",
      "        [294.0309, 117.0692, 343.2987, 162.4099],\n",
      "        [132.8911, 133.6766, 173.5143, 174.9349],\n",
      "        [285.5498, 119.6727, 462.2359, 185.3049],\n",
      "        [407.3842, 137.3996, 453.6654, 157.8846],\n",
      "        [135.3589, 124.5407, 171.9694, 144.4407],\n",
      "        [268.7044, 122.2878, 359.0063, 156.4207],\n",
      "        [335.2924, 149.9687, 401.3313, 275.8968],\n",
      "        [275.6942, 129.8365, 362.4065, 279.3526],\n",
      "        [261.6015, 114.0033, 300.6762, 142.3726],\n",
      "        [ 39.0525, 136.3273,  91.4837, 157.8284],\n",
      "        [ 66.2637,  95.5804, 364.0214, 214.6712],\n",
      "        [197.2861, 109.9367, 364.5493, 172.7354],\n",
      "        [112.5149, 120.2661, 144.6513, 142.1451],\n",
      "        [251.2532, 142.8585, 374.3412, 169.1632],\n",
      "        [ 51.8036, 104.8630, 194.1195, 160.1941],\n",
      "        [264.9545, 116.5659, 274.2256, 146.7078],\n",
      "        [303.2422, 122.5135, 382.3929, 155.5408],\n",
      "        [389.2437, 139.7047, 434.1999, 159.8160],\n",
      "        [ 42.2044, 115.9489,  60.3350, 162.0184],\n",
      "        [ 95.1181, 103.7529, 276.9344, 161.5305],\n",
      "        [215.0375, 135.8693, 331.1646, 161.2140],\n",
      "        [ 65.8311, 130.4151, 177.2846, 156.6399],\n",
      "        [175.5045, 125.7495, 359.0490, 200.1596],\n",
      "        [ 56.6760, 112.1840,  76.6384, 146.3877],\n",
      "        [182.9866, 120.8573, 202.5710, 169.4685],\n",
      "        [117.0685, 121.8096, 136.8606, 162.0939],\n",
      "        [333.6514, 124.5245, 431.5935, 155.8698],\n",
      "        [289.0355, 108.9102, 380.8177, 203.8865],\n",
      "        [ 17.3051, 134.6122,  42.6029, 158.5452],\n",
      "        [205.2641, 120.6803, 223.8105, 168.8629],\n",
      "        [167.6973, 132.9405, 264.6430, 164.6578],\n",
      "        [381.9245, 124.8492, 480.5246, 155.6531],\n",
      "        [ 18.5737, 133.3103, 201.7373, 207.3137],\n",
      "        [ 70.0575, 125.4967,  89.7537, 160.6860],\n",
      "        [ 10.1002, 118.3242,  26.5947, 160.2315],\n",
      "        [278.5017, 115.5723, 293.7618, 144.2267],\n",
      "        [271.3802, 133.9964, 385.9059, 162.5721],\n",
      "        [223.5793, 141.9020, 237.8627, 155.6347],\n",
      "        [309.4656, 123.1796, 336.5738, 153.9837],\n",
      "        [159.0259, 125.9920, 167.0260, 141.3000],\n",
      "        [300.2875, 124.4056, 344.4085, 145.2205],\n",
      "        [ 29.2935, 149.1138, 176.4950, 173.9920],\n",
      "        [362.1999, 129.8092, 441.1668, 160.0984],\n",
      "        [ 34.1608, 113.9822,  55.0645, 155.2890],\n",
      "        [ 17.9058, 127.9338,  95.2842, 158.6114],\n",
      "        [303.7718, 129.6076, 345.5716, 275.2317],\n",
      "        [143.1228, 121.6540, 158.8621, 138.9602],\n",
      "        [282.1386, 113.6967, 373.0441, 141.3989],\n",
      "        [409.9037, 128.5568, 433.1572, 155.7338],\n",
      "        [394.8948, 135.8664, 482.2975, 162.9687],\n",
      "        [319.3151, 190.9394, 352.2951, 276.3060],\n",
      "        [141.3987, 119.2171, 165.9581, 153.4496],\n",
      "        [283.5751, 162.0643, 414.8637, 196.8932],\n",
      "        [277.7495, 133.5193, 332.8470, 155.3912],\n",
      "        [418.8777, 138.5467, 488.8729, 152.0922],\n",
      "        [157.2446,  55.8870, 424.5201, 213.7667],\n",
      "        [127.9652, 132.5166, 196.6296, 160.6754],\n",
      "        [102.9477, 133.5130, 322.9819, 211.5265],\n",
      "        [158.2052, 131.0238, 188.1962, 159.5185],\n",
      "        [263.8862, 112.5271, 285.1484, 153.0264]], device='cuda:0')), scores: tensor([0.9582, 0.8548, 0.8431, 0.8313, 0.8269, 0.8088, 0.7601, 0.7491, 0.7399,\n",
      "        0.7306, 0.7002, 0.6803, 0.6649, 0.6613, 0.6554, 0.6311, 0.6291, 0.6194,\n",
      "        0.6183, 0.6104, 0.5968, 0.5950, 0.5895, 0.5717, 0.5677, 0.5558, 0.5262,\n",
      "        0.5261, 0.5073, 0.4988, 0.4838, 0.4812, 0.4762, 0.4726, 0.4683, 0.4644,\n",
      "        0.4626, 0.4488, 0.4480, 0.4469, 0.4460, 0.4357, 0.4338, 0.4266, 0.4239,\n",
      "        0.4169, 0.4168, 0.4136, 0.4044, 0.4038, 0.3946, 0.3915, 0.3577, 0.3572,\n",
      "        0.3456, 0.3449, 0.3392, 0.3319, 0.3315, 0.3279, 0.3117, 0.3005, 0.2956,\n",
      "        0.2915, 0.2884, 0.2855, 0.2817, 0.2807, 0.2756, 0.2671, 0.2654, 0.2581,\n",
      "        0.2573, 0.2570, 0.2489, 0.2476, 0.2441, 0.2423, 0.2392, 0.2387, 0.2359,\n",
      "        0.2262, 0.2259, 0.2203, 0.2198, 0.2171, 0.2016, 0.1910, 0.1878, 0.1775,\n",
      "        0.1774, 0.1736, 0.1701, 0.1669, 0.1642, 0.1614, 0.1613, 0.1581, 0.1522,\n",
      "        0.1512], device='cuda:0'), pred_classes: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')])}\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.evaluator:Start inference on 239 images\n",
      "INFO:fsdet.evaluation.evaluator:Inference done 50/239. 0.0523 s / img. ETA=0:00:09\n",
      "INFO:fsdet.evaluation.evaluator:Inference done 100/239. 0.0529 s / img. ETA=0:00:07\n",
      "INFO:fsdet.evaluation.evaluator:Inference done 150/239. 0.0530 s / img. ETA=0:00:04\n",
      "INFO:fsdet.evaluation.evaluator:Inference done 200/239. 0.0530 s / img. ETA=0:00:02\n",
      "INFO:fsdet.evaluation.evaluator:Total inference time: 0:00:12 (0.051282 s / img per device, on 1 devices)\n",
      "INFO:fsdet.evaluation.evaluator:Total inference pure compute time: 0:00:12 (0.051817 s / img per device, on 1 devices)\n",
      "INFO:fsdet.evaluation.coco_evaluation:Preparing results for COCO format ...\n",
      "INFO:fsdet.evaluation.coco_evaluation:Saving results to checkpoints/coco/faster_rcnn/test_0215/coco_instances_results.json\n",
      "INFO:fsdet.evaluation.coco_evaluation:Evaluating predictions ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.coco_evaluation:Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.497 | 11.452 | 0.857  | 2.916 | 2.782 | 6.296 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=1.14s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.115\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.063\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('bbox',\n",
       "              {'AP': 3.4974809343450457,\n",
       "               'AP50': 11.452398492432149,\n",
       "               'AP75': 0.8571714524642489,\n",
       "               'APs': 2.9159147665604803,\n",
       "               'APm': 2.781923164794646,\n",
       "               'APl': 6.295968797067547})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default test \n",
    "inference_on_dataset(model, data_loader_test, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run detector step by step (should have same results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([3, 800, 1166])\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0]['image'].shape)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(cfg.MODEL.PIXEL_MEAN) == len(cfg.MODEL.PIXEL_STD)\n",
    "num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "device = 'cuda'\n",
    "pixel_mean = (\n",
    "    torch.Tensor(cfg.MODEL.PIXEL_MEAN)\n",
    "    .to(device)\n",
    "    .view(num_channels, 1, 1)\n",
    ")\n",
    "pixel_std = (\n",
    "    torch.Tensor(cfg.MODEL.PIXEL_STD)\n",
    "    .to(device)\n",
    "    .view(num_channels, 1, 1)\n",
    ")\n",
    "normalizer = lambda x: (x - pixel_mean) / pixel_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.coco_evaluation:Preparing results for COCO format ...\n",
      "INFO:fsdet.evaluation.coco_evaluation:Saving results to checkpoints/coco/faster_rcnn/test_0215/coco_instances_results.json\n",
      "INFO:fsdet.evaluation.coco_evaluation:Evaluating predictions ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.coco_evaluation:Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 3.497 | 11.452 | 0.857  | 2.916 | 2.782 | 6.296 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=1.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.07s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.115\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.063\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.092\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.157\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.310\n"
     ]
    }
   ],
   "source": [
    "evaluator = COCOEvaluator(\"1000shot_person_test\", cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "evaluator.reset()\n",
    "\n",
    "training = False\n",
    "save_results = True\n",
    "box2box_transform = Box2BoxTransform(\n",
    "            weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS\n",
    "        )\n",
    "smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "test_score_thresh        = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "# test_score_thresh        = 0.7\n",
    "test_nms_thresh          = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "test_detections_per_img  = cfg.TEST.DETECTIONS_PER_IMAGE\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(data_loader_test):\n",
    "        #     batched_inputs = data\n",
    "        batched_inputs = inputs\n",
    "        ###################################    \n",
    "        #     outputs = model(inputs)     #\n",
    "        #---------------------------------# \n",
    "\n",
    "        # Normalize, pad and batch the input images. (Preprocess_image)\n",
    "        images = [x[\"image\"].to('cuda') for x in batched_inputs]\n",
    "        images = [normalizer(x) for x in images]\n",
    "        images = ImageList.from_tensors(\n",
    "            images, model.backbone.size_divisibility\n",
    "        )\n",
    "\n",
    "        # forward\n",
    "        features = model.backbone(images.tensor)\n",
    "#         print('features shape:', features['p3'].shape)\n",
    "        proposals, _ = model.proposal_generator(images, features)\n",
    "#         print('proposal num per img:', proposals_list[0].objectness_logits.shape)\n",
    "\n",
    "\n",
    "        #     results, _ = model.roi_heads(images, features, proposals)\n",
    "        #     print('\\ninstance for image 0:', results[0], '\\n')\n",
    "\n",
    "        # run roi_heads step by step\n",
    "        if training:\n",
    "        #         proposals = [proposal for proposal in proposals]\n",
    "            targets = [d['instances'].to('cuda') for d in data]\n",
    "            proposals = model.roi_heads.label_and_sample_proposals(proposals, targets)\n",
    "\n",
    "        box_features = model.roi_heads.box_pooler(\n",
    "            [features[f] for f in [\"p2\", \"p3\", \"p4\", \"p5\"]], [x.proposal_boxes for x in proposals]\n",
    "        )\n",
    "#         print(box_features.shape)\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "#         print(box_features.shape)\n",
    "\n",
    "        pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(\n",
    "            box_features\n",
    "        )\n",
    "#         print('pred_class_logits', pred_class_logits[:3])\n",
    "#         print('pred_proposal_deltas', pred_proposal_deltas.shape)\n",
    "\n",
    "        outputs = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        results, _ = outputs.inference(\n",
    "                test_score_thresh,\n",
    "                test_nms_thresh,\n",
    "                test_detections_per_img,\n",
    "        )\n",
    "\n",
    "        # postprocess: resize images\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            results, batched_inputs, images.image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"instances\": r})\n",
    "#         print('postprocessed instance for image 0:\\n', processed_results[0], '\\n')\n",
    "###################################  \n",
    "\n",
    "        # evaluate\n",
    "        evaluator.process(inputs, processed_results)\n",
    "        # visualizer\n",
    "        # inputs should be only one image\n",
    "        raw_image = cv2.imread(batched_inputs[0]['file_name'])\n",
    "        result_show = processed_results[0][\"instances\"]\n",
    "        v = Visualizer(raw_image,\n",
    "                        metadata=MetadataCatalog.get(\"1000shot_person_test\"), \n",
    "                        scale=1.0, \n",
    "                        instance_mode=ColorMode.IMAGE   # remove the colors of unsegmented pixels\n",
    "            )\n",
    "        v = v.draw_instance_predictions(result_show.to(\"cpu\"))\n",
    "        if save_results:\n",
    "            folder_name = './test_output/det/'\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            det_img_dir = folder_name + str(idx) + '.jpg'\n",
    "            cv2.imwrite(det_img_dir, v.get_image())\n",
    "    eval_results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add SVM classifier as the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.coco_evaluation:Preparing results for COCO format ...\n",
      "INFO:fsdet.evaluation.coco_evaluation:Saving results to checkpoints/coco/faster_rcnn/test_0215/coco_instances_results.json\n",
      "INFO:fsdet.evaluation.coco_evaluation:Evaluating predictions ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.coco_evaluation:Evaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 4.675 | 14.536 | 1.162  | 3.389 | 5.262 | 11.502 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=0.64s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.145\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.115\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.129\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.156\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.072\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.138\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.326\n"
     ]
    }
   ],
   "source": [
    "evaluator = COCOEvaluator(\"1000shot_person_test\", cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "evaluator.reset()\n",
    "training = False\n",
    "box2box_transform = Box2BoxTransform(\n",
    "            weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS\n",
    "        )\n",
    "smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "# test_score_thresh        = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "test_score_thresh        = 0.5\n",
    "test_nms_thresh          = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "test_detections_per_img  = cfg.TEST.DETECTIONS_PER_IMAGE\n",
    "print(test_score_thresh, test_nms_thresh, test_detections_per_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(data_loader_test):\n",
    "        #     batched_inputs = data\n",
    "        batched_inputs = inputs\n",
    "    ###################################    \n",
    "    #     outputs = model(inputs)     #\n",
    "    #---------------------------------# \n",
    "\n",
    "        # Normalize, pad and batch the input images. (Preprocess_image)\n",
    "        images = [x[\"image\"].to('cuda') for x in batched_inputs]\n",
    "        images = [normalizer(x) for x in images]\n",
    "        images = ImageList.from_tensors(\n",
    "            images, model.backbone.size_divisibility\n",
    "        )\n",
    "\n",
    "        # forward\n",
    "        features = model.backbone(images.tensor)\n",
    "#         print('features shape:', features['p3'].shape)\n",
    "        proposals, _ = model.proposal_generator(images, features)\n",
    "#         print('proposal num per img:', proposals_list[0].objectness_logits.shape)\n",
    "\n",
    "\n",
    "    #     results, _ = model.roi_heads(images, features, proposals)\n",
    "    #     print('\\ninstance for image 0:', results[0], '\\n')\n",
    "\n",
    "        # run roi_heads step by step\n",
    "        if training:\n",
    "    #         proposals = [proposal for proposal in proposals]\n",
    "            targets = [d['instances'].to('cuda') for d in data]\n",
    "            proposals = model.roi_heads.label_and_sample_proposals(proposals, targets)\n",
    "\n",
    "        box_features = model.roi_heads.box_pooler(\n",
    "            [features[f] for f in [\"p2\", \"p3\", \"p4\", \"p5\"]], [x.proposal_boxes for x in proposals]\n",
    "        )\n",
    "#         print(box_features.shape)\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "#         print(box_features.shape)\n",
    "\n",
    "        pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(\n",
    "            box_features\n",
    "        )\n",
    "#         print('pred_class_logits', pred_class_logits[:3])\n",
    "#         print('pred_proposal_deltas', pred_proposal_deltas.shape)\n",
    "\n",
    "        outputs = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        results, _ = outputs.inference(\n",
    "                test_score_thresh,\n",
    "                test_nms_thresh,\n",
    "                test_detections_per_img,\n",
    "        )\n",
    "\n",
    "        # postprocess: resize images\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            results, batched_inputs, images.image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"instances\": r})\n",
    "#         print('postprocessed instance for image 0:\\n', processed_results[0], '\\n')\n",
    "\n",
    "        # SVM \n",
    "        X = box_features.to('cpu').detach().numpy()\n",
    "#         y_hat = clf.predict(X)\n",
    "        pred_class_logits_svm = clf.predict_log_proba(X)\n",
    "#         pred_class_logits_svm = np.zeros((pred_class_logits.shape))\n",
    "#         for i in range(y_hat.shape[0]):\n",
    "#             # if y_hat=1, background; if y_hat=0, person\n",
    "#             pred_class_logits_svm[i][0] = 1 - y_hat[i]\n",
    "#             pred_class_logits_svm[i][-1] = y_hat[i]\n",
    "        pred_class_logits_svm = torch.from_numpy(pred_class_logits_svm).to('cuda')\n",
    "#         print(y_hat.shape)\n",
    "#         print(pred_class_logits_svm[:3])\n",
    "\n",
    "        outputs_svm = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits_svm,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        pred_instances_svm, _ = outputs_svm.inference(\n",
    "                test_score_thresh,\n",
    "                test_nms_thresh,\n",
    "                test_detections_per_img,\n",
    "        )\n",
    "\n",
    "        processed_results_svm = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            pred_instances_svm, batched_inputs, images.image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results_svm.append({\"instances\": r})\n",
    "#         print('\\n\\nSVM postprocessed instance for image 0:\\n', processed_results_svm[0], '\\n')  \n",
    "    \n",
    "###################################  \n",
    "        # evaluate\n",
    "        evaluator.process(inputs, processed_results_svm)\n",
    "        # visualizer\n",
    "        # inputs should be only one image\n",
    "        raw_image = cv2.imread(batched_inputs[0]['file_name'])\n",
    "        result_show = processed_results_svm[0][\"instances\"]\n",
    "        v = Visualizer(raw_image,\n",
    "                        metadata=MetadataCatalog.get(\"1000shot_person_test\"), \n",
    "                        scale=1.0, \n",
    "                        instance_mode=ColorMode.IMAGE   # remove the colors of unsegmented pixels\n",
    "            )\n",
    "        v = v.draw_instance_predictions(result_show.to(\"cpu\"))\n",
    "        if save_results:\n",
    "#             det_img_folder = time.strftime(\"%d_%H_%M/\")\n",
    "            folder_name = './test_output/svm_prob/'\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            det_img_dir = folder_name + str(idx) + '.jpg'\n",
    "            cv2.imwrite(det_img_dir, v.get_image())\n",
    "    results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsdet",
   "language": "python",
   "name": "fsdet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
