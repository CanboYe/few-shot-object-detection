{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n",
      "** fvcore version of PathManager will be deprecated soon. **\n",
      "** Please migrate to the version in iopath repo. **\n",
      "https://github.com/facebookresearch/iopath \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetMapper\n",
    "from detectron2.data import (\n",
    "    build_detection_test_loader,\n",
    "    build_detection_train_loader,\n",
    ")\n",
    "from detectron2.data.samplers import TrainingSampler\n",
    "from detectron2.modeling.proposal_generator.proposal_utils import add_ground_truth_to_proposals\n",
    "from detectron2.modeling.sampling import subsample_labels\n",
    "from detectron2.structures import Boxes, Instances, pairwise_iou\n",
    "from detectron2.modeling.matcher import Matcher\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "from fsdet.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from fsdet.evaluation import (\n",
    "    COCOEvaluator, DatasetEvaluators, LVISEvaluator, PascalVOCDetectionEvaluator, verify_results)\n",
    "from fsdet.evaluation import (\n",
    "    DatasetEvaluator,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    "    verify_results,\n",
    ")\n",
    "import cv2\n",
    "import torch, torchvision\n",
    "import logging\n",
    "import os\n",
    "from sklearn import svm\n",
    "from joblib import dump, load\n",
    "from detectron2.layers import nonzero_tuple\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained detection model and Prediect directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models of DNN and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fvcore.common.checkpoint:Loading checkpoint from checkpoints/coco/faster_rcnn/30shot_person_unfreeze_whole/model_0015999.pth\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file('configs/test_unfreeze_lastfews.yaml')\n",
    "model = build_model(cfg)  # returns a torch.nn.Module\n",
    "\n",
    "model.eval()\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "# ckpt_file = 'checkpoints/coco/base_model/model_final.pth'\n",
    "# ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_person_freeze_last_cos/model_final.pth'\n",
    "# ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_person_freeze_last_fc/model_final.pth'\n",
    "# ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_person_unfreeze_lastfews/model_final.pth'\n",
    "ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_person_unfreeze_whole/model_0015999.pth'\n",
    "# ckpt_file = 'checkpoints/coco/faster_rcnn/30shot_airplane_unfreeze_whole/model_0029999.pth'\n",
    "DetectionCheckpointer(model).load(ckpt_file)\n",
    "\n",
    "# clf = load('svm_results/svm_model_finetuned_prop_base.joblib') \n",
    "# clf = load('svm_results/svm_model_finetuned_prop_lastfew.joblib') \n",
    "# clf = load('svm_results/svm_model_finetuned_prop_whole.joblib') \n",
    "run_rcnn = False\n",
    "ft_extractor_type = ckpt_file.split('/')[-2]\n",
    "shots_num = 100\n",
    "class_name = 'person'\n",
    "file_name = 'svm_{}_{}_{}.joblib'.format(ft_extractor_type, shots_num, class_name)\n",
    "clf = load('svm_results_0216/'+ file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some functions for preprocessing\n",
    "assert len(cfg.MODEL.PIXEL_MEAN) == len(cfg.MODEL.PIXEL_STD)\n",
    "num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "device = 'cuda'\n",
    "pixel_mean = (\n",
    "    torch.Tensor(cfg.MODEL.PIXEL_MEAN)\n",
    "    .to(device)\n",
    "    .view(num_channels, 1, 1)\n",
    ")\n",
    "pixel_std = (\n",
    "    torch.Tensor(cfg.MODEL.PIXEL_STD)\n",
    "    .to(device)\n",
    "    .view(num_channels, 1, 1)\n",
    ")\n",
    "normalizer = lambda x: (x - pixel_mean) / pixel_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "json_dir = 'datasets/coco_experiments/seed1/full_box_30shot_person_trainval.json'\n",
    "image_dir = 'datasets/coco/trainval2014'\n",
    "register_coco_instances(\"30shot_person_train\", {}, json_dir, image_dir)\n",
    "\n",
    "json_dir = 'datasets/coco_experiments/seed1/full_box_{}shot_{}_test.json'.format(1000, class_name)\n",
    "image_dir = 'datasets/coco/trainval2014'\n",
    "testset_name = \"{}_test\".format(class_name)\n",
    "register_coco_instances(testset_name, {}, json_dir, image_dir)\n",
    "\n",
    "# json_dir = 'datasets/cocosplit/datasplit/5k.json'\n",
    "# image_dir = 'datasets/coco/trainval2014'\n",
    "# register_coco_instances(\"5k_test\", {}, json_dir, image_dir)\n",
    "\n",
    "# testset_name = \"5k_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between some dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_train = build_detection_train_loader(cfg)\n",
    "# data_loader_train_it = iter(data_loader_train)\n",
    "# data = next(data_loader_train_it)\n",
    "# print(len(data))\n",
    "# print(data[0]['image'].shape)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:detectron2.data.datasets.coco:Loaded 239 images in COCO format from datasets/coco_experiments/seed1/full_box_1000shot_person_test.json\n",
      "INFO:detectron2.data.build:Distribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   person   | 1000         |\n",
      "|            |              |\u001b[0m\n",
      "INFO:detectron2.data.dataset_mapper:[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "INFO:detectron2.data.common:Serializing 239 elements to byte tensors and concatenating them all ...\n",
      "INFO:detectron2.data.common:Serialized dataset takes 0.67 MiB\n"
     ]
    }
   ],
   "source": [
    "# data_loader_test = build_detection_test_loader(cfg, \"1000shot_person_test\")\n",
    "# data_loader_test_it = iter(data_loader_test)\n",
    "data_loader_test = build_detection_test_loader(cfg, testset_name)\n",
    "data_loader_test_it = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([3, 800, 1166])\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "data = next(data_loader_test_it)\n",
    "print(len(data))\n",
    "print(data[0]['image'].shape)\n",
    "print(len(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_gen = T.ResizeShortestEdge(\n",
    "#             [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],\n",
    "#             cfg.INPUT.MAX_SIZE_TEST,\n",
    "#             )\n",
    "# print([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST],\n",
    "#             cfg.INPUT.MAX_SIZE_TEST)\n",
    "# data_loader = build_detection_train_loader(DatasetCatalog.get(\"1000shot_person_test\"),\n",
    "#                                            mapper=DatasetMapper(cfg, is_train=False, augmentations=[transform_gen]),\n",
    "#                                            total_batch_size = 10)\n",
    "# data_loader_it = iter(data_loader)\n",
    "# data = next(data_loader_it)\n",
    "# print(len(data))\n",
    "# print(data[0]['image'].shape)\n",
    "# print(data)\n",
    "\n",
    "## the type of dataloader is different 'AspectRatio Grouped Dataset' (don't know the reason) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run one iter\n",
    "# evaluator = COCOEvaluator(\"1000shot_person_test\", cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "# with torch.no_grad():\n",
    "#     inputs = data\n",
    "#     outputs = model(inputs)\n",
    "#     evaluator.reset()\n",
    "#     evaluator.process(inputs, outputs)\n",
    "#     results = evaluator.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default test \n",
    "# run_rcnn = True\n",
    "if run_rcnn:\n",
    "    evaluator = COCOEvaluator(testset_name, cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "    evaluator.reset()\n",
    "    inference_on_dataset(model, data_loader_test, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run detector step by step (should have same results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_rcnn:\n",
    "    evaluator = COCOEvaluator(testset_name, cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "    evaluator.reset()\n",
    "\n",
    "    training = False\n",
    "    save_results = False\n",
    "    box2box_transform = Box2BoxTransform(\n",
    "                weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS\n",
    "            )\n",
    "    smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "    # test_score_thresh        = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "    test_score_thresh        = 0.5\n",
    "    test_nms_thresh          = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "    test_detections_per_img  = cfg.TEST.DETECTIONS_PER_IMAGE\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, inputs in enumerate(data_loader_test):\n",
    "            #     batched_inputs = data\n",
    "            batched_inputs = inputs\n",
    "            ###################################    \n",
    "            #     outputs = model(inputs)     #\n",
    "            #---------------------------------# \n",
    "\n",
    "            # Normalize, pad and batch the input images. (Preprocess_image)\n",
    "            images = [x[\"image\"].to('cuda') for x in batched_inputs]\n",
    "            images = [normalizer(x) for x in images]\n",
    "            images = ImageList.from_tensors(\n",
    "                images, model.backbone.size_divisibility\n",
    "            )\n",
    "\n",
    "            # forward\n",
    "            features = model.backbone(images.tensor)\n",
    "    #         print('features shape:', features['p3'].shape)\n",
    "            proposals, _ = model.proposal_generator(images, features)\n",
    "    #         print('proposal num per img:', proposals_list[0].objectness_logits.shape)\n",
    "\n",
    "\n",
    "            #     results, _ = model.roi_heads(images, features, proposals)\n",
    "            #     print('\\ninstance for image 0:', results[0], '\\n')\n",
    "\n",
    "            # run roi_heads step by step\n",
    "            if training:\n",
    "            #         proposals = [proposal for proposal in proposals]\n",
    "                targets = [d['instances'].to('cuda') for d in data]\n",
    "                proposals = model.roi_heads.label_and_sample_proposals(proposals, targets)\n",
    "\n",
    "            box_features = model.roi_heads.box_pooler(\n",
    "                [features[f] for f in [\"p2\", \"p3\", \"p4\", \"p5\"]], [x.proposal_boxes for x in proposals]\n",
    "            )\n",
    "    #         print(box_features.shape)\n",
    "            box_features = model.roi_heads.box_head(box_features)\n",
    "    #         print(box_features.shape)\n",
    "\n",
    "            pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(\n",
    "                box_features\n",
    "            )\n",
    "    #         print('pred_class_logits', pred_class_logits[:3])\n",
    "    #         print('pred_proposal_deltas', pred_proposal_deltas.shape)\n",
    "\n",
    "            outputs = FastRCNNOutputs(\n",
    "                box2box_transform,\n",
    "                pred_class_logits,\n",
    "                pred_proposal_deltas,\n",
    "                proposals,\n",
    "                smooth_l1_beta,\n",
    "            )\n",
    "\n",
    "            results, _ = outputs.inference(\n",
    "                    test_score_thresh,\n",
    "                    test_nms_thresh,\n",
    "                    test_detections_per_img,\n",
    "            )\n",
    "\n",
    "            # postprocess: resize images\n",
    "            processed_results = []\n",
    "            for results_per_image, input_per_image, image_size in zip(\n",
    "                results, batched_inputs, images.image_sizes\n",
    "            ):\n",
    "                height = input_per_image.get(\"height\", image_size[0])\n",
    "                width = input_per_image.get(\"width\", image_size[1])\n",
    "                r = detector_postprocess(results_per_image, height, width)\n",
    "                processed_results.append({\"instances\": r})\n",
    "    #         print('postprocessed instance for image 0:\\n', processed_results[0], '\\n')\n",
    "    ###################################  \n",
    "\n",
    "            # evaluate\n",
    "            evaluator.process(inputs, processed_results)\n",
    "            save_results = True\n",
    "            if save_results:\n",
    "                # visualizer\n",
    "                # inputs should be only one image\n",
    "                raw_image = cv2.imread(batched_inputs[0]['file_name'])\n",
    "                result_show = processed_results[0][\"instances\"]\n",
    "                v = Visualizer(raw_image,\n",
    "                                metadata=MetadataCatalog.get(testset_name), \n",
    "                                scale=1.0, \n",
    "                                instance_mode=ColorMode.IMAGE   # remove the colors of unsegmented pixels\n",
    "                    )\n",
    "                v = v.draw_instance_predictions(result_show.to(\"cpu\"))\n",
    "\n",
    "                folder_name = './test_0216/det_last_fc_0.5/'\n",
    "                os.makedirs(folder_name, exist_ok=True)\n",
    "                det_img_dir = folder_name + str(idx) + '.jpg'\n",
    "                cv2.imwrite(det_img_dir, v.get_image())\n",
    "        eval_results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add SVM classifier as the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.5 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cloudlet/work/few-shot-object-detection/fsdet/modeling/roi_heads/fast_rcnn.py:115: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370116979/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  filter_inds = filter_mask.nonzero()\n",
      "INFO:fsdet.evaluation.coco_evaluation:Preparing results for COCO format ...\n",
      "INFO:fsdet.evaluation.coco_evaluation:Saving results to checkpoints/coco/faster_rcnn/test_0215/coco_instances_results.json\n",
      "INFO:fsdet.evaluation.coco_evaluation:Evaluating predictions ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fsdet.evaluation.coco_evaluation:Evaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
      "| 18.337 | 42.884 | 12.980 | 10.582 | 23.060 | 30.587 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE (t=0.59s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.183\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.429\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.130\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.306\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.106\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.268\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.303\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.170\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.329\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.491\n",
      "Now trying to plot PR curve\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluator = COCOEvaluator(testset_name, cfg, True, output_dir = cfg.OUTPUT_DIR)\n",
    "evaluator.reset()\n",
    "training = False\n",
    "box2box_transform = Box2BoxTransform(\n",
    "            weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS\n",
    "        )\n",
    "smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "# test_score_thresh        = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "test_score_thresh        = 0.5\n",
    "test_nms_thresh          = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "test_detections_per_img  = cfg.TEST.DETECTIONS_PER_IMAGE\n",
    "print(test_score_thresh, test_nms_thresh, test_detections_per_img)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, inputs in enumerate(data_loader_test):\n",
    "        #     batched_inputs = data\n",
    "        batched_inputs = inputs\n",
    "    ###################################    \n",
    "    #     outputs = model(inputs)     #\n",
    "    #---------------------------------# \n",
    "\n",
    "        # Normalize, pad and batch the input images. (Preprocess_image)\n",
    "        images = [x[\"image\"].to('cuda') for x in batched_inputs]\n",
    "        images = [normalizer(x) for x in images]\n",
    "        images = ImageList.from_tensors(\n",
    "            images, model.backbone.size_divisibility\n",
    "        )\n",
    "\n",
    "        # forward\n",
    "        features = model.backbone(images.tensor)\n",
    "#         print('features shape:', features['p3'].shape)\n",
    "        proposals, _ = model.proposal_generator(images, features)\n",
    "#         print('proposal num per img:', proposals[0].objectness_logits.shape) # 1000 proposals\n",
    "\n",
    "\n",
    "    #     results, _ = model.roi_heads(images, features, proposals)\n",
    "    #     print('\\ninstance for image 0:', results[0], '\\n')\n",
    "\n",
    "        # run roi_heads step by step\n",
    "        if training:\n",
    "    #         proposals = [proposal for proposal in proposals]\n",
    "            targets = [x['instances'].to('cuda') for x in batched_inputs]\n",
    "            proposals = model.roi_heads.label_and_sample_proposals(proposals, targets)\n",
    "\n",
    "        box_features = model.roi_heads.box_pooler(\n",
    "            [features[f] for f in [\"p2\", \"p3\", \"p4\", \"p5\"]], [x.proposal_boxes for x in proposals]\n",
    "        )\n",
    "#         print(box_features.shape)\n",
    "        box_features = model.roi_heads.box_head(box_features)\n",
    "#         print(box_features.shape)\n",
    "\n",
    "        pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(\n",
    "            box_features\n",
    "        )\n",
    "#         print('pred_class_logits', pred_class_logits[:3])\n",
    "#         print('pred_proposal_deltas', pred_proposal_deltas.shape)\n",
    "\n",
    "        outputs = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        results, _ = outputs.inference(\n",
    "                test_score_thresh,\n",
    "                test_nms_thresh,\n",
    "                test_detections_per_img,\n",
    "        )\n",
    "\n",
    "        # postprocess: resize images\n",
    "        processed_results = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            results, batched_inputs, images.image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results.append({\"instances\": r})\n",
    "#         print('postprocessed instance for image 0:\\n', processed_results[0], '\\n')\n",
    "\n",
    "        # SVM \n",
    "        X = box_features.to('cpu').detach().numpy()\n",
    "#         y_hat = clf.predict(X)\n",
    "        pred_class_logits_svm = clf.predict_log_proba(X)\n",
    "#         pred_class_logits_svm = np.zeros((pred_class_logits.shape))\n",
    "#         for i in range(y_hat.shape[0]):\n",
    "#             # if y_hat=1, background; if y_hat=0, person\n",
    "#             pred_class_logits_svm[i][0] = 1 - y_hat[i]\n",
    "#             pred_class_logits_svm[i][-1] = y_hat[i]\n",
    "        pred_class_logits_svm = torch.from_numpy(pred_class_logits_svm).to('cuda')\n",
    "#         print(y_hat.shape)\n",
    "#         print(pred_class_logits_svm[:3])\n",
    "\n",
    "        outputs_svm = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits_svm,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        pred_instances_svm, _ = outputs_svm.inference(\n",
    "                test_score_thresh,\n",
    "                test_nms_thresh,\n",
    "                test_detections_per_img,\n",
    "        )\n",
    "\n",
    "        processed_results_svm = []\n",
    "        for results_per_image, input_per_image, image_size in zip(\n",
    "            pred_instances_svm, batched_inputs, images.image_sizes\n",
    "        ):\n",
    "            height = input_per_image.get(\"height\", image_size[0])\n",
    "            width = input_per_image.get(\"width\", image_size[1])\n",
    "            r = detector_postprocess(results_per_image, height, width)\n",
    "            processed_results_svm.append({\"instances\": r})\n",
    "#         print('\\n\\nSVM postprocessed instance for image 0:\\n', processed_results_svm[0], '\\n')  \n",
    "    \n",
    "###################################  \n",
    "        # evaluate\n",
    "        evaluator.process(inputs, processed_results_svm)\n",
    "        save_results = False\n",
    "        if save_results:\n",
    "            # visualizer\n",
    "            # inputs should be only one image\n",
    "            raw_image = cv2.imread(batched_inputs[0]['file_name'])\n",
    "            result_show = processed_results_svm[0][\"instances\"]\n",
    "            v = Visualizer(raw_image,\n",
    "                            metadata=MetadataCatalog.get(\"1000shot_person_test\"), \n",
    "                            scale=1.0, \n",
    "                            instance_mode=ColorMode.IMAGE   # remove the colors of unsegmented pixels\n",
    "                )\n",
    "            v = v.draw_instance_predictions(result_show.to(\"cpu\"))\n",
    "        \n",
    "#             det_img_folder = time.strftime(\"%d_%H_%M/\")\n",
    "            folder_name = './test_0216/svm_{}_{}_{}_{}/'.format(ft_extractor_type, shots_num, class_name, test_score_thresh)\n",
    "            os.makedirs(folder_name, exist_ok=True)\n",
    "            det_img_dir = folder_name + str(idx) + '.jpg'\n",
    "            cv2.imwrite(det_img_dir, v.get_image())\n",
    "    results = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsdet",
   "language": "python",
   "name": "fsdet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
